{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_document = r'C:\\Users\\Jahnavi\\Documents\\3rd_Year\\Y3S2\\[E1TA2] ECE352\\IoT_Project\\Speech\\Dataset\\output.csv'\n",
    "\n",
    "# Try to read the CSV file without headers\n",
    "try:\n",
    "    test_doc = pd.read_csv(test_document, header=None)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"No data in {test_document}\")\n",
    "    test_doc = pd.DataFrame()\n",
    "\n",
    "# If the DataFrame is not empty, rename the column and convert to lowercase\n",
    "if not test_doc.empty:\n",
    "    test_doc.columns = ['text']\n",
    "    test_doc['text'] = test_doc['text'].str.lower()\n",
    "else:\n",
    "    print(\"The DataFrame is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't downloaded the tokenizer package, uncomment the line below to download\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "if not test_doc.empty:\n",
    "    test_doc['tokenized_text'] = test_doc['text'].apply(tokenize_text)\n",
    "else:\n",
    "    print(\"The DataFrame is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [2, 3, 1, 4, 5, 6, 7, 1, 8, 9, 10, 11, 12, 13,...\n",
      "Name: encoded_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text\n",
    "# This will create the vocabulary\n",
    "if not test_doc.empty:\n",
    "    tokenizer.fit_on_texts(test_doc['tokenized_text'].tolist())\n",
    "\n",
    "# Convert the tokens into integers\n",
    "test_doc['encoded_text'] = test_doc['tokenized_text'].apply(lambda x: tokenizer.texts_to_sequences([x])[0])\n",
    "\n",
    "print(test_doc['encoded_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [2, 3, 1, 4, 5, 6, 7, 1, 8, 9, 10, 11, 12, 13,...\n",
      "Name: padded_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sequences\n",
    "# This will make all sequences the same length\n",
    "if not test_doc.empty:\n",
    "    test_doc['padded_text'] = pad_sequences(test_doc['encoded_text'].tolist()).tolist()\n",
    "\n",
    "print(test_doc['padded_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 909ms/step\n",
      "[[0.02683993 0.87918985 0.04624833 0.00636089 0.0368391  0.00452189]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Convert the 'padded_text' column to a numpy array\n",
    "X = np.array(test_doc['padded_text'].tolist())\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy']\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to map indices to class names\n",
    "index_to_class = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear'\n",
    "}\n",
    "\n",
    "# Find the index of the maximum probability for each sequence\n",
    "max_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the indices to class names\n",
    "predicted_classes = [index_to_class[index] for index in max_indices]\n",
    "\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy Counter({'joy': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each class\n",
    "counter = Counter(predicted_classes)\n",
    "\n",
    "# Find the class with the highest count\n",
    "overall_class = counter.most_common(1)[0][0]\n",
    "\n",
    "print(overall_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "import ssl\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Check if the overall class is 'sadness'\n",
    "    # Email settings\n",
    "subject = \"Emotional Diagnosis of Patient\"\n",
    "body = \"The patient is predicted to be feeling \" + overall_class + \". Please find the attached CSV file for more details on your patient.\"\n",
    "sender_email = \"yeruvasai.jahnavi2021@vitstudent.ac.in\"\n",
    "receiver_email = \"yeruvasai.jahnavi2021@vitstudent.ac.in\"\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "filename = r\"C:\\Users\\Jahnavi\\Documents\\3rd_Year\\Y3S2\\[E1TA2] ECE352\\IoT_Project\\Speech\\Dataset\\output.csv\"\n",
    "# Create a multipart message\n",
    "msg = MIMEMultipart()\n",
    "msg[\"From\"] = sender_email\n",
    "msg[\"To\"] = receiver_email\n",
    "msg[\"Subject\"] = subject\n",
    "# Add the email body\n",
    "msg.attach(MIMEText(body, \"plain\"))\n",
    "# Open the file in binary mode\n",
    "with open(filename, \"rb\") as attachment:\n",
    "    # Add file as application/octet-stream\n",
    "    part = MIMEBase(\"application\", \"octet-stream\")\n",
    "    part.set_payload(attachment.read())\n",
    "# Encode file in ASCII characters to send by email    \n",
    "encoders.encode_base64(part)\n",
    "# Add header as pdf attachment\n",
    "part.add_header(\n",
    "    \"Content-Disposition\",\n",
    "    f\"attachment; filename= {filename}\",\n",
    ")\n",
    "# Add attachment to message and convert message to string\n",
    "msg.attach(part)\n",
    "text = msg.as_string()\n",
    "# Log in to server using secure context and send email\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, receiver_email, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
